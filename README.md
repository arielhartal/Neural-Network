# Deep Learning from Scratch: Neural Network Implementation and Optimization

This project aims to provide an in-depth understanding of how deep learning frameworks work behind the scenes by building a simple neural network from scratch. The project is divided into two parts and focuses on various essential concepts, such as backpropagation, stochastic optimization methods, hyperparameter tuning, and the integration of different neural network architectures.

The project involves designing and implementing a neural network for the classification of small vector data sets. Starting with a labeled training dataset, the goal is to create a network architecture that can predict labels for new data points. The project emphasizes learning how to implement custom networks, optimize them using stochastic gradient descent (SGD) and its variants, and test these networks against validation data.

# Key Objectives
Understanding Deep Learning Frameworks: Gain a solid grasp of backpropagation, hyperparameters, and optimization, which are the foundation of deep learning models.
Scientific Programming: Develop experience in building custom optimization algorithms, understanding their moving parts, and implementing effective solutions.
Hands-On with Neural Networks: Build both standard feedforward neural networks and residual neural networks (ResNets) from scratch.
Training and Classification: Train the neural network to classify data samples, optimize weights, and assess model performance using gradient tests and softmax regression.
